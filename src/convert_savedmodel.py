"""
convert_savedmodel.py - Convierte usando SavedModel como intermediario
"""
import tensorflow as tf
import os
from pathlib import Path

print("="*60)
print("CONVERSIÃ“N VÃA SAVEDMODEL")
print("="*60)

# Paso 1: Cargar el H5 y guardarlo como SavedModel
print("\nğŸ“¦ Paso 1: Cargando modelo H5...")

h5_path = '../models/model_tomato_disease.h5'
savedmodel_path = '../models/saved_model_temp'

try:
    # Cargar con compile=False para evitar problemas
    model = tf.keras.models.load_model(h5_path, compile=False)
    print("âœ“ Modelo H5 cargado (sin compilar)")
    
    # Guardarlo como SavedModel
    print("\nğŸ’¾ Paso 2: Guardando como SavedModel...")
    model.save(savedmodel_path, save_format='tf')
    print(f"âœ“ SavedModel guardado en: {savedmodel_path}")
    
except Exception as e:
    print(f"âœ— Error: {e}")
    print("\nâš ï¸  Vamos a intentar otro mÃ©todo...")
    
    # MÃ©todo alternativo: cargar arquitectura y pesos por separado
    print("\nğŸ”„ MÃ©todo alternativo: ReconstrucciÃ³n inteligente...")
    
    import h5py
    import json
    
    # Leer la configuraciÃ³n del modelo del H5
    with h5py.File(h5_path, 'r') as f:
        # Obtener la configuraciÃ³n del modelo si existe
        if 'model_config' in f.attrs:
            model_config = json.loads(f.attrs['model_config'])
            print("âœ“ ConfiguraciÃ³n del modelo encontrada")
            
            # Reconstruir desde la configuraciÃ³n
            from tensorflow.keras.models import model_from_json
            model = model_from_json(json.dumps(model_config))
            
            # Cargar pesos
            model.load_weights(h5_path)
            print("âœ“ Modelo reconstruido y pesos cargados")
            
            # Guardar como SavedModel
            model.save(savedmodel_path, save_format='tf')
            print(f"âœ“ SavedModel guardado en: {savedmodel_path}")
        else:
            print("âœ— No se pudo leer la configuraciÃ³n del modelo")
            exit(1)

# Paso 3: Convertir SavedModel a TFLite
print("\nğŸ”„ Paso 3: Convirtiendo a TFLite...")

try:
    converter = tf.lite.TFLiteConverter.from_saved_model(savedmodel_path)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    
    tflite_model = converter.convert()
    
    tflite_path = '../models/model_tomato_quantized.tflite'
    with open(tflite_path, 'wb') as f:
        f.write(tflite_model)
    
    # EstadÃ­sticas
    h5_size = Path(h5_path).stat().st_size / (1024 * 1024)
    tflite_size = len(tflite_model) / (1024 * 1024)
    
    print("\n" + "="*60)
    print("âœ“ CONVERSIÃ“N EXITOSA")
    print("="*60)
    print(f"\nâœ“ Modelo TFLite guardado en: {tflite_path}")
    print(f"\nğŸ“Š EstadÃ­sticas:")
    print(f"  TamaÃ±o H5: {h5_size:.2f} MB")
    print(f"  TamaÃ±o TFLite: {tflite_size:.2f} MB")
    print(f"  ReducciÃ³n: {((h5_size - tflite_size) / h5_size * 100):.1f}%")
    
    # Limpiar archivo temporal
    import shutil
    if os.path.exists(savedmodel_path):
        shutil.rmtree(savedmodel_path)
        print("\nğŸ§¹ Archivo temporal eliminado")
    
    print("\n" + "="*60)
    print("ğŸ‰ Â¡LISTO PARA USAR!")
    print("="*60)
    print("\nğŸ“‹ PrÃ³ximo paso:")
    print("  cd ..")
    print("  streamlit run streamlit_app.py")
    
except Exception as e:
    print(f"\nâœ— Error durante conversiÃ³n: {e}")
    import traceback
    traceback.print_exc()